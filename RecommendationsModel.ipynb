{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "f7ebf237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'John', 'age': 30, 'car': 'None'}\n",
      "{'name': b'Chilled Prune Whip', 'userId': b'3526'}\n",
      "WARNING:tensorflow:vocab_size is deprecated, please use vocabulary_size.\n",
      "WARNING:tensorflow:vocab_size is deprecated, please use vocabulary_size.\n",
      "Epoch 1/5\n",
      "3/3 [==============================] - 1s 112ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0012 - factorized_top_k/top_5_categorical_accuracy: 0.0077 - factorized_top_k/top_10_categorical_accuracy: 0.0181 - factorized_top_k/top_50_categorical_accuracy: 0.1186 - factorized_top_k/top_100_categorical_accuracy: 0.3409 - loss: 23860.1987 - regularization_loss: 0.0000e+00 - total_loss: 23860.1987      \n",
      "Epoch 2/5\n",
      "3/3 [==============================] - 0s 108ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0498 - factorized_top_k/top_5_categorical_accuracy: 0.2593 - factorized_top_k/top_10_categorical_accuracy: 0.3395 - factorized_top_k/top_50_categorical_accuracy: 0.6068 - factorized_top_k/top_100_categorical_accuracy: 0.7713 - loss: 22293.7939 - regularization_loss: 0.0000e+00 - total_loss: 22293.7939\n",
      "Epoch 3/5\n",
      "3/3 [==============================] - 0s 106ms/step - factorized_top_k/top_1_categorical_accuracy: 0.2612 - factorized_top_k/top_5_categorical_accuracy: 0.6590 - factorized_top_k/top_10_categorical_accuracy: 0.7366 - factorized_top_k/top_50_categorical_accuracy: 0.8824 - factorized_top_k/top_100_categorical_accuracy: 0.9411 - loss: 19808.0645 - regularization_loss: 0.0000e+00 - total_loss: 19808.0645\n",
      "Epoch 4/5\n",
      "3/3 [==============================] - 0s 111ms/step - factorized_top_k/top_1_categorical_accuracy: 0.3187 - factorized_top_k/top_5_categorical_accuracy: 0.8669 - factorized_top_k/top_10_categorical_accuracy: 0.9179 - factorized_top_k/top_50_categorical_accuracy: 0.9699 - factorized_top_k/top_100_categorical_accuracy: 0.9860 - loss: 14575.9927 - regularization_loss: 0.0000e+00 - total_loss: 14575.9927\n",
      "Epoch 5/5\n",
      "3/3 [==============================] - 0s 110ms/step - factorized_top_k/top_1_categorical_accuracy: 0.3039 - factorized_top_k/top_5_categorical_accuracy: 0.9466 - factorized_top_k/top_10_categorical_accuracy: 0.9786 - factorized_top_k/top_50_categorical_accuracy: 0.9956 - factorized_top_k/top_100_categorical_accuracy: 0.9989 - loss: 12904.4182 - regularization_loss: 0.0000e+00 - total_loss: 12904.4182\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow_recommenders.layers.factorized_top_k.BruteForce at 0x127eafabfd0>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Dict, Text\n",
    "import pprint\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_recommenders as tfrs\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "\n",
    "json_string = '{ \"name\":\"John\", \"age\":30, \"car\":\"None\" }'\n",
    "your_json = json.loads(json_string)\n",
    "print(your_json)\n",
    "\n",
    "recipes = pd.read_csv(\"recipes.csv\")\n",
    "users = pd.read_csv(\"users.csv\")\n",
    "\n",
    "recipes = recipes.astype({\"recipeId\": str, \"name\": str})\n",
    "users = users.astype({\"userId\": str, \"recipeId\": str, \"name\": str})\n",
    "\n",
    "newColumn = []\n",
    "\n",
    "\n",
    "# for i in recipes['categories']:\n",
    "#     temp = i.split(\" | \")\n",
    "#     newColumn.append(temp)\n",
    "\n",
    "# recipes.drop(columns=['categories'])\n",
    "# recipes[\"categories\"] = newColumn\n",
    "# print(recipes)\n",
    "\n",
    "# print(recipes.dtypes)\n",
    "\n",
    "recipes = tf.data.Dataset.from_tensor_slices(dict(recipes))\n",
    "users = tf.data.Dataset.from_tensor_slices(dict(users))\n",
    "\n",
    "users = users.map(lambda x: {\n",
    "    \"name\": x[\"name\"],\n",
    "    \"userId\": x[\"userId\"]\n",
    "})\n",
    "\n",
    "recipes = recipes.map(lambda x: x[\"name\"])\n",
    "                      \n",
    "# for x in users.take(1).as_numpy_iterator():\n",
    "#   pprint.pprint(x)\n",
    "\n",
    "user_ids_vocabulary = tf.keras.layers.StringLookup(mask_token=None)\n",
    "user_ids_vocabulary.adapt(users.map(lambda x: x[\"userId\"]))\n",
    "\n",
    "recipe_titles_vocabulary = tf.keras.layers.StringLookup(mask_token=None)\n",
    "recipe_titles_vocabulary.adapt(recipes)\n",
    "\n",
    "\n",
    "class RecipeModel(tfrs.Model):\n",
    "\n",
    "  def __init__(\n",
    "      self,\n",
    "      user_model: tf.keras.Model,\n",
    "      recipe_model: tf.keras.Model,\n",
    "      task: tfrs.tasks.Retrieval):\n",
    "    super().__init__()\n",
    "\n",
    "    self.user_model = user_model\n",
    "    self.recipe_model = recipe_model\n",
    "    self.task = task\n",
    "\n",
    "  def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
    "    user_embeddings = self.user_model(features[\"userId\"])\n",
    "    positive_recipe_embeddings = self.recipe_model(features[\"name\"])\n",
    "    return self.task(user_embeddings, positive_recipe_embeddings)\n",
    "\n",
    "user_model = tf.keras.Sequential([\n",
    "    user_ids_vocabulary,\n",
    "    tf.keras.layers.Embedding(user_ids_vocabulary.vocab_size(), 64)\n",
    "])\n",
    "recipe_model = tf.keras.Sequential([\n",
    "    recipe_titles_vocabulary,\n",
    "    tf.keras.layers.Embedding(recipe_titles_vocabulary.vocab_size(), 64)\n",
    "])\n",
    "\n",
    "task = tfrs.tasks.Retrieval(metrics=tfrs.metrics.FactorizedTopK(\n",
    "    recipes.batch(128).map(recipe_model)\n",
    "  )\n",
    ")\n",
    "\n",
    "model = RecipeModel(user_model, recipe_model, task)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.5))\n",
    "\n",
    "model.fit(users.batch(4096), epochs=5)\n",
    "\n",
    "index = tfrs.layers.factorized_top_k.BruteForce(model.user_model)\n",
    "index.index_from_dataset(\n",
    "    recipes.batch(100).map(lambda title: (title, model.recipe_model(title))))\n",
    "\n",
    "# Get some recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "b0a4bb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_recommendations = {}\n",
    "\n",
    "for i in range(0, 9999):\n",
    "    _, titles = index(np.array([str(i)]))\n",
    "    temp = []\n",
    "    for j in titles[0, :6]:\n",
    "        temp.append(j.numpy().decode(\"utf-8\"))\n",
    "    final_recommendations[str(i)] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "1f89492d",
   "metadata": {},
   "outputs": [],
   "source": [
    "app_json = json.dumps(final_recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "081600e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"recommendations.json\", \"w\") as outfile:\n",
    "    outfile.write(app_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
